{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import zipfile\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import zeros\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy import sparse\n",
    "\n",
    "# pip install gensim\n",
    "import gensim\n",
    "import gensim.models\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"../data/train_data_cleaning.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['content_ready'] = train_data['content_ready'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"../data/test_data_cleaning.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['content_ready'] = test_data['content_ready'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf-idf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(analyzer=lambda x: x)\n",
    "\n",
    "# Fit the vectorizer to the documents and transform the documents into TF-IDF vectors\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(train_data['content_ready'])\n",
    "X_test_tfidf = tfidf_vectorizer.transform(test_data['content_ready'])\n",
    "\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse.save_npz('../data/X_train_tfidf.npz', X_train_tfidf)\n",
    "sparse.save_npz('../data/X_test_tfidf.npz', X_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_vec(words, w2vec_model):\n",
    "    \"\"\"\n",
    "    Function to take a document as a list of words \n",
    "    and return the document vector\n",
    "    \n",
    "    Arg:\n",
    "        - words: a list of words\n",
    "        - w2vec_model: vector of vocabularies\n",
    "    \"\"\"\n",
    "    presented_words = []\n",
    "    for word in words:\n",
    "        if word in w2vec_model.wv.key_to_index:\n",
    "            presented_words.append(word)\n",
    "    if presented_words:\n",
    "        word_vectors = [w2vec_model.wv[word] for word in presented_words]\n",
    "        return np.mean(word_vectors, axis=0)\n",
    "    else: return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Word2Vec Model with the corpus\n",
    "corpus = train_data['content_ready']\n",
    "\n",
    "size_vect = 100\n",
    "size_window = 15\n",
    "ch_sg = 1 # skip-gram\n",
    "min_word_cnt = 10\n",
    "\n",
    "# build the model with the entire corpus\n",
    "model = gensim.models.word2vec.Word2Vec(corpus\n",
    "                            , min_count = min_word_cnt\n",
    "                            , vector_size = size_vect\n",
    "                            , window = size_window\n",
    "                            , sg = ch_sg\n",
    "                            , workers = 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['w2vec'] = train_data['content_ready'].apply(lambda sent : get_doc_vec(sent, model))\n",
    "test_data['w2vec'] = test_data['content_ready'].apply(lambda sent : get_doc_vec(sent, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [-0.021934662, -0.045313407, -0.20713057, -0.0...\n",
       "1       [-0.01478289, 0.21770811, -0.21053442, 0.06472...\n",
       "2       [-0.010699491, -0.011163153, -0.246315, 0.0106...\n",
       "3       [-0.023718074, 0.046120156, -0.12290713, -0.04...\n",
       "4       [-0.16364671, 0.061136227, -0.16094913, -0.043...\n",
       "                              ...                        \n",
       "9131    [-0.10141687, -0.30549508, -0.54992586, 0.0652...\n",
       "9132    [0.0094024455, -0.055430166, -0.007694498, 0.0...\n",
       "9133    [0.117975764, 0.11803543, 0.029490937, -0.0583...\n",
       "9134    [0.017669259, -0.12720896, -0.18396588, -0.150...\n",
       "9135    [-0.05806205, 0.023231583, -0.34358263, 0.1442...\n",
       "Name: w2vec, Length: 9136, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['w2vec']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v_normalization(x_data, w2vec_column, size_vector):\n",
    "    '''\n",
    "    x_data: either x_train or x_val\n",
    "    y_data: either y_train or y_val\n",
    "    '''\n",
    "    # Data Normalization\n",
    "    x_np_vecs = np.zeros((len(x_data), size_vector))\n",
    "    for i, vec in enumerate(w2vec_column):\n",
    "        x_np_vecs[i, :] = vec\n",
    "\n",
    "    # Combine the full dataframe with the labels\n",
    "    x_data_w2v = pd.DataFrame(data = x_np_vecs\n",
    "                              , index = x_data.index)\n",
    "\n",
    "    # Join train data with label data in order to remove NaN values\n",
    "    x_data_w2v = x_data_w2v.dropna()\n",
    "    return x_data_w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_w2v = w2v_normalization(train_data['content_ready'], train_data['w2vec'], size_vect)\n",
    "x_test_w2v = w2v_normalization(test_data['content_ready'], test_data['w2vec'], size_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrames to CSV\n",
    "x_train_w2v.to_csv('../data/x_train_w2v.csv', index=False)\n",
    "x_test_w2v.to_csv('../data/x_test_w2v.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_embeddings = dict()\n",
    "\n",
    "with zipfile.ZipFile('../data/glove.6B.100d.txt.zip', 'r') as zip_ref:\n",
    "    with zip_ref.open('glove.6B.100d.txt') as f:\n",
    "        for line in f:\n",
    "            values = line.decode().split()\n",
    "            word = values[0]\n",
    "            coefs = [float(val) for val in values[1:]]\n",
    "            glove_embeddings[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "print('Loaded %s word vectors.' % len(glove_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to map words to GloVe embeddings\n",
    "def map_words_to_embeddings(text, embeddings_index, embedding_dim):\n",
    "    embedded_text = []\n",
    "    for word in text:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedded_text.append(embedding_vector)\n",
    "    if embedded_text:\n",
    "        return np.mean(embedded_text, axis=0)  # Average word embeddings in the text\n",
    "    else:\n",
    "        return np.zeros(embedding_dim)  # Use zero vector if no embeddings found\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_train_data = []\n",
    "for text in train_data['content_ready']:\n",
    "    vectorized_text = map_words_to_embeddings(text, glove_embeddings, 100)\n",
    "    vectorized_train_data.append(vectorized_text)\n",
    "\n",
    "vectorized_train_data = np.array(vectorized_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_test_data = []\n",
    "for text in test_data['content_ready']:\n",
    "    vectorized_text = map_words_to_embeddings(text, glove_embeddings, 100)\n",
    "    vectorized_test_data.append(vectorized_text)\n",
    "\n",
    "vectorized_test_data = np.array(vectorized_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed('../data/vectorized_train_data.npz', \n",
    "                    data=vectorized_train_data)\n",
    "\n",
    "np.savez_compressed('../data/vectorized_test_data.npz', \n",
    "                    data=vectorized_test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
